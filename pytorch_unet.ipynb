{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","execution_count":null,"source":["import torch\r\n","import torch.nn as nn\r\n","import torchvision.transforms.functional as TF\r\n","import glob\r\n","import pandas as pd\r\n","import matplotlib.pyplot as plt\r\n","import cv2\r\n","from PIL import Image"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-17T19:43:07.083904Z","iopub.execute_input":"2021-08-17T19:43:07.084255Z","iopub.status.idle":"2021-08-17T19:43:08.420212Z","shell.execute_reply.started":"2021-08-17T19:43:07.084176Z","shell.execute_reply":"2021-08-17T19:43:08.419342Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["class DoubleConv(nn.Module):\r\n","    def __init__(self,in_channels,out_channels):\r\n","        super(DoubleConv, self).__init__()\r\n","        self.conv = nn.Sequential(\r\n","            nn.Conv2d(in_channels,out_channels,3,1,1,bias=False),\r\n","            nn.BatchNorm2d(out_channels),\r\n","            nn.ReLU(inplace=True),\r\n","            nn.Conv2d(out_channels,out_channels,3,1,1,bias=False),\r\n","            nn.BatchNorm2d(out_channels),\r\n","            nn.ReLU(inplace=True)\r\n","        )\r\n","\r\n","    def forward(self, x):\r\n","        return self.conv(x)\r\n","\r\n","class UNET(nn.Module):\r\n","    def __init__(\r\n","        self,in_channels=3,out_channels=1,features=[64,128,256,512]\r\n","\r\n","    ):\r\n","        super(UNET,self).__init__()\r\n","        self.downs = nn.ModuleList()\r\n","        self.ups = nn.ModuleList()\r\n","        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\r\n","\r\n","        for feature in features:\r\n","            self.downs.append(DoubleConv(in_channels,feature))\r\n","            in_channels = feature\r\n","\r\n","        for feature in reversed(features):\r\n","            self.ups.append(\r\n","                nn.ConvTranspose2d(\r\n","                    feature*2,feature,kernel_size=2,stride=2\r\n","                )\r\n","            )\r\n","            self.ups.append(DoubleConv(feature*2,feature))\r\n","\r\n","        self.bottleneck = DoubleConv(features[-1],features[-1]*2)\r\n","\r\n","        self.final_conv = nn.Conv2d(features[0],out_channels,kernel_size=1)\r\n","\r\n","    def forward(self,x):\r\n","        skip_connections = []\r\n","        for down in self.downs:\r\n","            x = down(x)\r\n","            skip_connections.append(x)\r\n","            x = self.pool(x)\r\n","            \r\n","        x = self.bottleneck(x)\r\n","        skip_connections = skip_connections[::-1]\r\n","\r\n","        for idx in range(0, len(self.ups), 2):\r\n","            x = self.ups[idx](x)\r\n","            skip_connection = skip_connections[idx//2]\r\n","\r\n","            if x.shape != skip_connection.shape:\r\n","                x = TF.resize(x, size=skip_connection.shape[2:])\r\n","\r\n","            concat_skip = torch.cat((skip_connection, x), dim=1)\r\n","            x = self.ups[idx+1](concat_skip)\r\n","\r\n","        return self.final_conv(x)\r\n","\r\n","def test():\r\n","    x = torch.randn((3, 1, 161, 161))\r\n","    model = UNET(in_channels=1, out_channels=1)\r\n","    preds = model(x)\r\n","    print(preds.shape)\r\n","    print(x.shape)\r\n","    assert preds.shape == x.shape\r\n","\r\n","if __name__ == \"__main__\":\r\n","    test()  "],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-17T19:43:08.421646Z","iopub.execute_input":"2021-08-17T19:43:08.421989Z","iopub.status.idle":"2021-08-17T19:43:10.477487Z","shell.execute_reply.started":"2021-08-17T19:43:08.421955Z","shell.execute_reply":"2021-08-17T19:43:10.476383Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["import os\r\n","from PIL import Image\r\n","from torch.utils.data import Dataset\r\n","import numpy as np\r\n","\r\n","class CarvanaDataset(Dataset):\r\n","    def __init__(self, image_dir, mask_dir, transform=None):\r\n","        self.image_dir = image_dir\r\n","        self.mask_dir = mask_dir\r\n","        self.transform = transform\r\n","        self.images = os.listdir(image_dir)\r\n","\r\n","    def __len__(self):\r\n","        return len(self.images)\r\n","\r\n","    def __getitem__(self, index):\r\n","        img_path = os.path.join(self.image_dir, self.images[index])\r\n","        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\r\n","        image = np.array(Image.open(img_path).convert(\"RGB\"))\r\n","        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\r\n","        mask[mask == 255.0] = 1.0\r\n","\r\n","        if self.transform is not None:\r\n","            augmentations = self.transform(image=image, mask=mask)\r\n","            image = augmentations[\"image\"]\r\n","            mask = augmentations[\"mask\"]\r\n","\r\n","        return image, mask"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-17T19:43:10.479319Z","iopub.execute_input":"2021-08-17T19:43:10.479839Z","iopub.status.idle":"2021-08-17T19:43:10.489301Z","shell.execute_reply.started":"2021-08-17T19:43:10.479797Z","shell.execute_reply":"2021-08-17T19:43:10.488365Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["import torch\r\n","import torchvision\r\n","from torch.utils.data import DataLoader\r\n","\r\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\r\n","    print(\"=> Saving checkpoint\")\r\n","    torch.save(state, filename)\r\n","\r\n","def load_checkpoint(checkpoint, model):\r\n","    print(\"=> Loading checkpoint\")\r\n","    model.load_state_dict(checkpoint[\"state_dict\"])\r\n","\r\n","def get_loaders(\r\n","    images_dir,\r\n","    images_maskdir,\r\n","    batch_size,\r\n","    train_transform,\r\n","    val_transform,\r\n","    num_workers=4,\r\n","    pin_memory=True,\r\n","):\r\n","    dataset = CarvanaDataset(\r\n","        image_dir=images_dir,\r\n","        mask_dir=images_maskdir,\r\n","        transform=train_transform,\r\n","    )\r\n","    \r\n","    images = (pd.read_csv('./train_masks.csv')['img'])\r\n","    train_size = int(len(images)*0.8)\r\n","    val_size = len(images)-train_size\r\n","    train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\r\n","    \r\n","    train_loader = DataLoader(train_set,\r\n","                              batch_size=batch_size,\r\n","                              num_workers=num_workers,\r\n","                              pin_memory=pin_memory,\r\n","                              shuffle=True)\r\n","\r\n","    val_loader = DataLoader(val_set,\r\n","                              batch_size=batch_size,\r\n","                              num_workers=num_workers,\r\n","                              pin_memory=pin_memory,\r\n","                              shuffle=True)\r\n","    \r\n","    return train_loader, val_loader\r\n","\r\n","def check_accuracy(loader, model, device=\"cuda\"):\r\n","    num_correct = 0\r\n","    num_pixels = 0\r\n","    dice_score = 0\r\n","    model.eval()\r\n","\r\n","    with torch.no_grad():\r\n","        for x, y in loader:\r\n","            x = x.to(device)\r\n","            y = y.to(device).unsqueeze(1)\r\n","            preds = torch.sigmoid(model(x))\r\n","            preds = (preds > 0.5).float()\r\n","            num_correct += (preds == y).sum()\r\n","            num_pixels += torch.numel(preds)\r\n","            dice_score += (2 * (preds * y).sum()) / (\r\n","                (preds + y).sum() + 1e-8\r\n","            )\r\n","\r\n","    print(\r\n","        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\r\n","    )\r\n","    print(f\"Dice score: {dice_score/len(loader)}\")\r\n","    model.train()\r\n","\r\n","def save_predictions_as_imgs(\r\n","    loader, model, folder=\"saved_images/\", device=\"cuda\"\r\n","):\r\n","    model.eval()\r\n","    for idx, (x, y) in enumerate(loader):\r\n","        x = x.to(device=device)\r\n","        with torch.no_grad():\r\n","            preds = torch.sigmoid(model(x))\r\n","            preds = (preds > 0.5).float()\r\n","        torchvision.utils.save_image(\r\n","            preds, f\"{folder}/pred_{idx}.png\"\r\n","        )\r\n","        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\r\n","\r\n","    model.train()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-17T19:43:10.492473Z","iopub.execute_input":"2021-08-17T19:43:10.492763Z","iopub.status.idle":"2021-08-17T19:43:10.512383Z","shell.execute_reply.started":"2021-08-17T19:43:10.492736Z","shell.execute_reply":"2021-08-17T19:43:10.511434Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# !unzip '/kaggle/input/carvana-image-masking-challenge/train_hq.zip' -d './'\r\n","# !unzip '/kaggle/input/carvana-image-masking-challenge/train_masks.zip' -d './'\r\n","# !unzip '/kaggle/input/carvana-image-masking-challenge/train.zip' -d './'\r\n","# !unzip '/kaggle/input/carvana-image-masking-challenge/train_masks.csv.zip' -d './'"],"outputs":[],"metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-17T19:43:10.513714Z","iopub.execute_input":"2021-08-17T19:43:10.514108Z","iopub.status.idle":"2021-08-17T19:43:37.76746Z","shell.execute_reply.started":"2021-08-17T19:43:10.514066Z","shell.execute_reply":"2021-08-17T19:43:37.766565Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["print('Train Size : ',len(glob.glob('./train/*')))\r\n","print('Mask Size : ',len(glob.glob('./train_masks/*')))\r\n","data = pd.read_csv('train_masks.csv')\r\n","data"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-17T19:43:37.770697Z","iopub.execute_input":"2021-08-17T19:43:37.770988Z","iopub.status.idle":"2021-08-17T19:43:38.230861Z","shell.execute_reply.started":"2021-08-17T19:43:37.770955Z","shell.execute_reply":"2021-08-17T19:43:38.230079Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["fig=plt.figure(figsize=(15, 15))\r\n","img=cv2.imread('./train/d46244bc42ed_04.jpg')\r\n","mask=Image.open('./train_masks/d46244bc42ed_04_mask.gif') #Image from PIL \r\n","print(img.shape)\r\n","files=[img,mask]\r\n","for i in range(len(files)):\r\n","    plt.subplot(1, 2 , i+1)\r\n","    plt.imshow(files[i])"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-17T19:43:38.232173Z","iopub.execute_input":"2021-08-17T19:43:38.232504Z","iopub.status.idle":"2021-08-17T19:43:39.340844Z","shell.execute_reply.started":"2021-08-17T19:43:38.232467Z","shell.execute_reply":"2021-08-17T19:43:39.339851Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["import torch\r\n","import albumentations as A\r\n","from albumentations.pytorch import ToTensorV2\r\n","from tqdm import tqdm\r\n","import torch.nn as nn\r\n","import torch.optim as optim\r\n","\r\n","# Hyperparameters etc.\r\n","LEARNING_RATE = 1e-4\r\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n","BATCH_SIZE = 16\r\n","NUM_EPOCHS = 3\r\n","NUM_WORKERS = 2\r\n","IMAGE_HEIGHT = 160  # 1280 originally\r\n","IMAGE_WIDTH = 240  # 1918 originally\r\n","PIN_MEMORY = True\r\n","LOAD_MODEL = False\r\n","TRAIN_IMG_DIR = \"./train/\"\r\n","TRAIN_MASK_DIR = \"./train_masks/\"\r\n","VAL_IMG_DIR = \"data/val_images/\"\r\n","VAL_MASK_DIR = \"data/val_masks/\"\r\n","IMG_DIR = \"./train/\"\r\n","MASK_DIR = \"./train_masks/\"\r\n","\r\n","def train_fn(loader, model, optimizer, loss_fn, scaler):\r\n","    loop = tqdm(loader)\r\n","\r\n","    for batch_idx, (data, targets) in enumerate(loop):\r\n","        data = data.to(device=DEVICE)\r\n","        targets = targets.float().unsqueeze(1).to(device=DEVICE)\r\n","\r\n","        # forward\r\n","        with torch.cuda.amp.autocast():\r\n","            predictions = model(data)\r\n","            loss = loss_fn(predictions, targets)\r\n","\r\n","        # backward\r\n","        optimizer.zero_grad()\r\n","        scaler.scale(loss).backward()\r\n","        scaler.step(optimizer)\r\n","        scaler.update()\r\n","\r\n","        # update tqdm loop\r\n","        loop.set_postfix(loss=loss.item())\r\n"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-17T19:43:39.343364Z","iopub.execute_input":"2021-08-17T19:43:39.343755Z","iopub.status.idle":"2021-08-17T19:43:41.155377Z","shell.execute_reply.started":"2021-08-17T19:43:39.343716Z","shell.execute_reply":"2021-08-17T19:43:41.15438Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["train_transform = A.Compose(\r\n","    [\r\n","        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\r\n","        A.Rotate(limit=35, p=1.0),\r\n","        A.HorizontalFlip(p=0.5),\r\n","        A.VerticalFlip(p=0.1),\r\n","        A.Normalize(\r\n","            mean=[0.0, 0.0, 0.0],\r\n","            std=[1.0, 1.0, 1.0],\r\n","            max_pixel_value=255.0,\r\n","        ),\r\n","        ToTensorV2(),\r\n","    ],\r\n",")\r\n","\r\n","val_transforms = A.Compose(\r\n","    [\r\n","        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\r\n","        A.Normalize(\r\n","            mean=[0.0, 0.0, 0.0],\r\n","            std=[1.0, 1.0, 1.0],\r\n","            max_pixel_value=255.0,\r\n","        ),\r\n","        ToTensorV2(),\r\n","    ],\r\n",")\r\n","\r\n","model = UNET(in_channels=3, out_channels=1).to(DEVICE)\r\n","loss_fn = nn.BCEWithLogitsLoss()\r\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\r\n","\r\n","train_loader, val_loader = get_loaders(\r\n","    IMG_DIR,\r\n","    MASK_DIR,\r\n","    BATCH_SIZE,\r\n","    train_transform,\r\n","    val_transforms,\r\n","    NUM_WORKERS,\r\n","    PIN_MEMORY,\r\n",")\r\n","\r\n","if LOAD_MODEL:\r\n","    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\r\n","\r\n","\r\n","check_accuracy(val_loader, model, device=DEVICE)\r\n","scaler = torch.cuda.amp.GradScaler()\r\n","\r\n","for epoch in range(NUM_EPOCHS):\r\n","    train_fn(train_loader, model, optimizer, loss_fn, scaler)\r\n","\r\n","    # save model\r\n","    checkpoint = {\r\n","        \"state_dict\": model.state_dict(),\r\n","        \"optimizer\":optimizer.state_dict(),\r\n","    }\r\n","    save_checkpoint(checkpoint)\r\n","\r\n","    # check accuracy\r\n","    check_accuracy(val_loader, model, device=DEVICE)\r\n"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-17T19:43:41.157118Z","iopub.execute_input":"2021-08-17T19:43:41.1575Z","iopub.status.idle":"2021-08-17T20:00:55.637617Z","shell.execute_reply.started":"2021-08-17T19:43:41.157461Z","shell.execute_reply":"2021-08-17T20:00:55.636444Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["!mkdir results"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-17T20:00:55.639342Z","iopub.execute_input":"2021-08-17T20:00:55.639798Z","iopub.status.idle":"2021-08-17T20:00:56.344299Z","shell.execute_reply.started":"2021-08-17T20:00:55.639749Z","shell.execute_reply":"2021-08-17T20:00:56.343266Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["for idx, (x, y) in enumerate(val_loader):\r\n","    x = x.to(device=DEVICE)\r\n","    with torch.no_grad():\r\n","        preds = torch.sigmoid(model(x))\r\n","        preds = (preds > 0.5).float()\r\n","    torchvision.utils.save_image(\r\n","        preds, f\"results/pred_{idx}.png\"\r\n","    )\r\n","    torchvision.utils.save_image(y.unsqueeze(1), f\"results/{idx}.png\")\r\n","    torchvision.utils.save_image(\r\n","        x, f\"results/original_{idx}.png\"\r\n","    )\r\n"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-17T20:00:56.345826Z","iopub.execute_input":"2021-08-17T20:00:56.346171Z","iopub.status.idle":"2021-08-17T20:01:51.98662Z","shell.execute_reply.started":"2021-08-17T20:00:56.346119Z","shell.execute_reply":"2021-08-17T20:01:51.985465Z"},"trusted":true}}]}